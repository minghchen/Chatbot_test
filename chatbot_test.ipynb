{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\") # ignore warnings for better demonstration\n",
    "import os\n",
    "import sys\n",
    "import random\n",
    "import time\n",
    "import numpy as np\n",
    "import math\n",
    "\n",
    "# Make tensorflow less verbose; filter out info (1+) and warnings (2+) but not errors (3).\n",
    "os.environ['TF_CPP_MIN_LOG_LEVEL'] = '2'\n",
    "import tensorflow as tf\n",
    "from tensorflow.python.platform import gfile\n",
    "from six.moves import xrange\n",
    "\n",
    "import data_utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#set config\n",
    "class gen_config(object):\n",
    "    initialize = True\n",
    "    learning_rate = 0.5\n",
    "    learning_rate_decay_factor = 0.99\n",
    "    batch_size = 128\n",
    "    emb_dim = 128 # 512\n",
    "    num_layers = 1 # 2\n",
    "    vocab_size = 20000\n",
    "    max_gradient_norm = 5.0\n",
    "    steps_per_checkpoint = 100 #200\n",
    "    pretrain_steps = 4000\n",
    "    train_dir = 'data' # 'movie_data'\n",
    "    save_dir = 'log/gen_models'\n",
    "    tensorboard_dir = 'log/tensorboard'\n",
    "    buckets = [(5, 10), (10, 15), (20, 25), (40, 50)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reading development and training gen_data\n"
     ]
    }
   ],
   "source": [
    "#prepare the data\n",
    "vocab, rev_vocab, dev_set, train_set = data_utils.prepare_data(gen_config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Build Craph\n",
    "class Seq2SeqModel(object):\n",
    "    def __init__(self, config, name_scope, forward_only=False, num_samples=512, dtype=tf.float32):\n",
    "        self.vocab_size = config.vocab_size\n",
    "        self.emb_dim = config.emb_dim\n",
    "        self.buckets = config.buckets\n",
    "        self.learning_rate = tf.Variable(float(config.learning_rate), name=\"learning_rate\", trainable=False, dtype=dtype)\n",
    "        self.learning_rate_decay_op = self.learning_rate.assign(self.learning_rate * config.learning_rate_decay_factor)\n",
    "        self.global_step = tf.Variable(0, name=\"global_step\", trainable=False)\n",
    "        self.batch_size = config.batch_size\n",
    "        self.num_layers = config.num_layers\n",
    "        self.max_gradient_norm = config.max_gradient_norm\n",
    "        self.forward_only = tf.placeholder(tf.bool, name=\"forward_only\")\n",
    "        # Feeds for inputs.\n",
    "        self.encoder_inputs = tf.placeholder(tf.int32, shape=[None, None], name=\"encoder_inputs\") # [seq_len, batch]\n",
    "        self.decoder_inputs = tf.placeholder(tf.int32, shape=[None, None], name=\"decoder_inputs\")\n",
    "        self.targets = tf.placeholder(tf.int32, shape=[None, None], name=\"targets\")\n",
    "        self.target_weights = tf.placeholder(tf.float32, shape=[None, None], name=\"target_weight\")\n",
    "        self.inputs_len = tf.placeholder(tf.int32, shape=[None])\n",
    "        self.target_len = tf.placeholder(tf.int32, shape=[None])\n",
    "        size = self.emb_dim\n",
    "\n",
    "        # Embedding\n",
    "        self.enc_embedding = tf.get_variable(\n",
    "                \"encoder_embedding\", [self.vocab_size, self.emb_dim], dtype=tf.float32)\n",
    "        embed_inputs = tf.nn.embedding_lookup(self.enc_embedding, self.encoder_inputs) # [seq_len, batch, emb_dim]\n",
    "        # Encoder\n",
    "        encoder_cell = tf.nn.rnn_cell.GRUCell(size)\n",
    "        if self.num_layers > 1:\n",
    "            encoder_cell = tf.nn.rnn_cell.MultiRNNCell([encoder_cell] * self.num_layers)\n",
    "        # Dynamic encoding\n",
    "        encoder_outputs, encoder_state = tf.nn.dynamic_rnn(\n",
    "            encoder_cell, embed_inputs, dtype=tf.float32, sequence_length=self.inputs_len, time_major=True)\n",
    "        \n",
    "        # Output projection layer\n",
    "        with tf.variable_scope(\"output_projection\"):\n",
    "            self.output_layer = tf.layers.Dense(self.vocab_size)\n",
    "            self.output_layer.build(size)\n",
    "            # w and b are used in sample_loss\n",
    "            w = self.output_layer.kernel\n",
    "            w_t = tf.transpose(w)\n",
    "            b = self.output_layer.bias\n",
    "\n",
    "        # Decoder\n",
    "        embed_targets = tf.nn.embedding_lookup(self.enc_embedding, self.decoder_inputs)\n",
    "        decoder_cell = tf.nn.rnn_cell.GRUCell(size)\n",
    "        if self.num_layers > 1:\n",
    "            decoder_cell = tf.nn.rnn_cell.MultiRNNCell([decoder_cell] * self.num_layers)\n",
    "        if not forward_only:\n",
    "            # teacher focusing\n",
    "            helper = tf.contrib.seq2seq.TrainingHelper(\n",
    "                embed_targets, self.target_len, time_major=True)\n",
    "        else:\n",
    "            start_tokens = tf.fill([self.batch_size], data_utils.GO_ID)\n",
    "            end_token = -1 # we dont need EOS to finish decoding(for compating with the shape of self.targets)\n",
    "            helper = tf.contrib.seq2seq.GreedyEmbeddingHelper(\n",
    "                self.enc_embedding, start_tokens, end_token)\n",
    "        decoder = tf.contrib.seq2seq.BasicDecoder(\n",
    "            decoder_cell, helper, encoder_state, output_layer=None if not forward_only else self.output_layer)\n",
    "\n",
    "        # Dynamic decoding\n",
    "        outputs, final_context_state, final_sequence_lengths = tf.contrib.seq2seq.dynamic_decode(\n",
    "            decoder,\n",
    "            maximum_iterations=None if not forward_only else tf.reduce_max(self.target_len),\n",
    "            output_time_major=True,\n",
    "            swap_memory=True)\n",
    "        self.sample_id = outputs.sample_id\n",
    "        self.logits = outputs.rnn_output\n",
    "        \n",
    "        # Loss\n",
    "        def sampled_loss(inputs, labels):\n",
    "            labels = tf.reshape(labels, [-1, 1])\n",
    "            return tf.cast(\n",
    "                tf.nn.sampled_softmax_loss(weights=w_t, biases=b, inputs=inputs, labels=labels,\n",
    "                                                num_sampled=num_samples, num_classes=self.vocab_size), dtype)\n",
    "        if not forward_only:\n",
    "            _, self.loss = tf.while_loop(lambda time, loss: tf.less(time, tf.reduce_max(self.target_len)),\n",
    "                          lambda time, loss: (time + 1, loss + tf.reduce_mean(sampled_loss(self.logits[time], self.targets[time])*self.target_weights[time])),\n",
    "                          loop_vars=[tf.constant(0), tf.constant(0.0, dtype=dtype)])\n",
    "        else:\n",
    "            crossent = tf.nn.sparse_softmax_cross_entropy_with_logits(\n",
    "                labels=self.targets, logits=self.logits)\n",
    "            self.loss = tf.reduce_sum(crossent * self.target_weights) / tf.to_float(self.batch_size)\n",
    "            \n",
    "        # Gradient Descent\n",
    "        params = tf.trainable_variables()\n",
    "        if not forward_only:\n",
    "            opt = tf.train.GradientDescentOptimizer(self.learning_rate)\n",
    "            gradients = tf.gradients(self.loss, params)\n",
    "            clipped_gradients, norm = tf.clip_by_global_norm(gradients, self.max_gradient_norm)\n",
    "            self.gradient_norm=norm\n",
    "            self.update=opt.apply_gradients(zip(clipped_gradients, params), global_step=self.global_step)\n",
    "\n",
    "        self.saver = tf.train.Saver(tf.global_variables())\n",
    "        \n",
    "    def step(self, session, encoder_inputs, decoder_inputs, target_weights, inputs_len, target_len,\n",
    "        bucket_id, forward_only):\n",
    "        encoder_size, decoder_size = self.buckets[bucket_id]\n",
    "        # Input feed: encoder inputs, decoder inputs, target_weights, as provided.\n",
    "        input_feed = {}\n",
    "        input_feed[self.encoder_inputs] = encoder_inputs\n",
    "        input_feed[self.decoder_inputs] = decoder_inputs\n",
    "        input_feed[self.target_weights] = target_weights\n",
    "        # Our targets are decoder inputs shifted by one.\n",
    "        input_feed[self.targets] = decoder_inputs[1:]+[np.zeros([self.batch_size], dtype=np.int32)]\n",
    "\n",
    "        input_feed[self.inputs_len] = inputs_len\n",
    "        input_feed[self.target_len] = np.ones([self.batch_size], dtype=np.int32)* decoder_size\n",
    "        # Output feed: depends on whether we do a backward step or not.\n",
    "        if not forward_only:\n",
    "            output_feed = [self.update,  # Update Op that does SGD.\n",
    "                         self.gradient_norm,  # Gradient norm.\n",
    "                         self.loss]  # Loss for this batch.\n",
    "        else:\n",
    "            output_feed = [self.loss]  # Loss for this batch.\n",
    "            for l in xrange(decoder_size if decoder_size<20 else 20):  # Output logits.\n",
    "                output_feed.append(self.sample_id[l])\n",
    "\n",
    "        outputs = session.run(output_feed, input_feed)\n",
    "        if not forward_only:\n",
    "            return outputs[1], outputs[2], None  # Gradient norm, loss, no outputs.\n",
    "        else:\n",
    "            return None, outputs[0], outputs[1:]  # No gradient norm, loss, outputs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def train(gen_config):\n",
    "    # creating and loading the vocabulary and the train and dev data\n",
    "    vocab, rev_vocab, dev_set, train_set = data_utils.prepare_data(gen_config)\n",
    "    for b_set in train_set:\n",
    "        print(\"b_set: \", len(b_set))\n",
    "\n",
    "    with tf.Session() as sess:\n",
    "        print(\"Creating %d layers of %d units.\" % (gen_config.num_layers, gen_config.emb_dim))\n",
    "        creat_time = time.time()\n",
    "        model = Seq2SeqModel(gen_config, name_scope=\"Basic_Seq2seq\", forward_only=False,\n",
    "                                        dtype=tf.float32)\n",
    "        sess.run(tf.variables_initializer(tf.global_variables()))\n",
    "        print(\"creat gen_model time: %.3f\" % (time.time()-creat_time))\n",
    "\n",
    "        train_bucket_sizes = [len(train_set[b]) for b in range(len(gen_config.buckets))]\n",
    "        train_total_size = float(sum(train_bucket_sizes))\n",
    "        train_buckets_scale = [sum(train_bucket_sizes[:i + 1]) / train_total_size\n",
    "                               for i in xrange(len(train_bucket_sizes))]\n",
    "\n",
    "        # This is the training loop.\n",
    "        step_time, loss = 0.0, 0.0\n",
    "        current_step = 0\n",
    "        previous_losses = []\n",
    "        \n",
    "        print(\"Begin training...\")\n",
    "        print(\"Record every %d steps\" % gen_config.steps_per_checkpoint)\n",
    "        while current_step < gen_config.pretrain_steps:\n",
    "            # Choose a bucket according to disc_data distribution. We pick a random number\n",
    "            # in [0, 1] and use the corresponding interval in train_buckets_scale.\n",
    "            random_number_01 = np.random.random_sample()\n",
    "            bucket_id = min([i for i in range(len(train_buckets_scale)) if train_buckets_scale[i] > random_number_01])\n",
    "\n",
    "            # Get a batch and make a step.\n",
    "            start_time = time.time()\n",
    "            encoder_inputs, decoder_inputs, target_weights, inputs_len, target_len = data_utils.get_batch(\n",
    "                gen_config, train_set, bucket_id)\n",
    "\n",
    "            _, step_loss, _ = model.step(sess, encoder_inputs, decoder_inputs, target_weights, inputs_len, target_len, \n",
    "                                            bucket_id, forward_only=False)\n",
    "\n",
    "            step_time += (time.time() - start_time) / gen_config.steps_per_checkpoint\n",
    "            loss += step_loss / gen_config.steps_per_checkpoint\n",
    "            current_step += 1\n",
    "            print(\"\\r step:{:5}  step_loss:{:8.4f} step_time:{:8.4f} bucket:{}\".format(current_step, step_loss, time.time() - start_time, bucket_id), end=' ')\n",
    "            # Once in a while, we print statistics.\n",
    "            if current_step % gen_config.steps_per_checkpoint == 0:\n",
    "                # Print statistics for the previous epoch.\n",
    "                perplexity = math.exp(loss) if loss < 300 else float('inf')\n",
    "                print(\"\\n global step %d learning rate %.4f step-time %.2f loss %.4f perplexity \"\n",
    "                      \"%.2e\" % (model.global_step.eval(), model.learning_rate.eval(),\n",
    "                                step_time, loss, perplexity))\n",
    "                # Decrease learning rate if no improvement was seen over last 3 times.\n",
    "                if len(previous_losses) > 2 and loss > max(previous_losses[-3:]):\n",
    "                    sess.run(model.learning_rate_decay_op)\n",
    "                previous_losses.append(loss)\n",
    "                step_time, loss = 0.0, 0.0\n",
    "                sys.stdout.flush()\n",
    "        # Save model\n",
    "        gen_ckpt_dir = os.path.abspath(os.path.join(gen_config.save_dir, \"checkpoints\"))\n",
    "        if not os.path.exists(gen_ckpt_dir):\n",
    "            os.makedirs(gen_ckpt_dir)\n",
    "        checkpoint_path = os.path.join(gen_ckpt_dir, \"gen.model\")\n",
    "        print(\"current_step: %d, save model to %s\" % (current_step, os.path.join(gen_config.save_dir, \"checkpoints\")))\n",
    "        model.saver.save(sess, checkpoint_path, global_step=model.global_step)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reading development and training gen_data\n",
      "b_set:  1785\n",
      "b_set:  16959\n",
      "b_set:  33353\n",
      "b_set:  20381\n",
      "Creating 1 layers of 128 units.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/chenminghao/miniconda3/lib/python3.6/site-packages/tensorflow/python/ops/gradients_impl.py:96: UserWarning: Converting sparse IndexedSlices to a dense Tensor of unknown shape. This may consume a large amount of memory.\n",
      "  \"Converting sparse IndexedSlices to a dense Tensor of unknown shape. \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "creat gen_model time: 0.955\n",
      "Begin training...\n",
      "Record every 100 steps\n",
      " step:  100  step_loss: 33.6349 step_time:  0.0575 bucket:0 \n",
      " global step 100 learning rate 0.5000 step-time 0.15 loss 89.9530 perplexity 1.16e+39\n",
      " step:  200  step_loss: 55.0414 step_time:  0.1313 bucket:2 \n",
      " global step 200 learning rate 0.5000 step-time 0.16 loss 72.6084 perplexity 3.42e+31\n",
      " step:  300  step_loss: 90.8312 step_time:  0.2689 bucket:3 \n",
      " global step 300 learning rate 0.5000 step-time 0.16 loss 65.1550 perplexity 1.98e+28\n",
      " step:  400  step_loss: 31.4943 step_time:  0.0807 bucket:1 \n",
      " global step 400 learning rate 0.5000 step-time 0.15 loss 59.0490 perplexity 4.41e+25\n",
      " step:  500  step_loss: 44.5948 step_time:  0.1295 bucket:2 \n",
      " global step 500 learning rate 0.5000 step-time 0.15 loss 57.1043 perplexity 6.31e+24\n",
      " step:  600  step_loss: 88.8566 step_time:  0.2693 bucket:3 \n",
      " global step 600 learning rate 0.5000 step-time 0.15 loss 54.5215 perplexity 4.77e+23\n",
      " step:  700  step_loss: 81.5079 step_time:  0.2722 bucket:3 \n",
      " global step 700 learning rate 0.5000 step-time 0.15 loss 51.9698 perplexity 3.72e+22\n",
      " step:  800  step_loss: 41.7919 step_time:  0.1295 bucket:2 \n",
      " global step 800 learning rate 0.5000 step-time 0.15 loss 52.0286 perplexity 3.94e+22\n",
      " step:  900  step_loss: 43.1019 step_time:  0.1275 bucket:2 \n",
      " global step 900 learning rate 0.5000 step-time 0.16 loss 54.6378 perplexity 5.36e+23\n",
      " step: 1000  step_loss: 45.2031 step_time:  0.1312 bucket:2 \n",
      " global step 1000 learning rate 0.4950 step-time 0.16 loss 52.2275 perplexity 4.81e+22\n",
      " step: 1100  step_loss: 43.7694 step_time:  0.1325 bucket:2 \n",
      " global step 1100 learning rate 0.4950 step-time 0.15 loss 48.8071 perplexity 1.57e+21\n",
      " step: 1200  step_loss: 46.7449 step_time:  0.1321 bucket:2 \n",
      " global step 1200 learning rate 0.4950 step-time 0.16 loss 50.7428 perplexity 1.09e+22\n",
      " step: 1300  step_loss: 29.5022 step_time:  0.0782 bucket:1 \n",
      " global step 1300 learning rate 0.4950 step-time 0.17 loss 52.3052 perplexity 5.20e+22\n",
      " step: 1400  step_loss: 82.5679 step_time:  0.2747 bucket:3 \n",
      " global step 1400 learning rate 0.4901 step-time 0.17 loss 51.5396 perplexity 2.42e+22\n",
      " step: 1500  step_loss: 23.4660 step_time:  0.0838 bucket:1 \n",
      " global step 1500 learning rate 0.4901 step-time 0.15 loss 47.1273 perplexity 2.93e+20\n",
      " step: 1600  step_loss: 25.1876 step_time:  0.0800 bucket:1 \n",
      " global step 1600 learning rate 0.4901 step-time 0.17 loss 52.1391 perplexity 4.40e+22\n",
      " step: 1700  step_loss: 71.4736 step_time:  0.2677 bucket:3 \n",
      " global step 1700 learning rate 0.4901 step-time 0.16 loss 48.0383 perplexity 7.29e+20\n",
      " step: 1800  step_loss: 39.8057 step_time:  0.1309 bucket:2 \n",
      " global step 1800 learning rate 0.4901 step-time 0.16 loss 47.7170 perplexity 5.29e+20\n",
      " step: 1900  step_loss: 71.3973 step_time:  0.2654 bucket:3  \n",
      " global step 1900 learning rate 0.4901 step-time 0.16 loss 46.0294 perplexity 9.78e+19\n",
      " step: 2000  step_loss: 38.4396 step_time:  0.1307 bucket:2 \n",
      " global step 2000 learning rate 0.4901 step-time 0.15 loss 44.1904 perplexity 1.55e+19\n",
      " step: 2100  step_loss: 69.8537 step_time:  0.2663 bucket:3 \n",
      " global step 2100 learning rate 0.4901 step-time 0.15 loss 44.0428 perplexity 1.34e+19\n",
      " step: 2200  step_loss: 22.4601 step_time:  0.0793 bucket:1 \n",
      " global step 2200 learning rate 0.4901 step-time 0.16 loss 45.4638 perplexity 5.55e+19\n",
      " step: 2300  step_loss: 42.0189 step_time:  0.1298 bucket:2 \n",
      " global step 2300 learning rate 0.4901 step-time 0.16 loss 45.9113 perplexity 8.69e+19\n",
      " step: 2400  step_loss: 39.0686 step_time:  0.1288 bucket:2 \n",
      " global step 2400 learning rate 0.4851 step-time 0.16 loss 44.7855 perplexity 2.82e+19\n",
      " step: 2500  step_loss: 38.1718 step_time:  0.1298 bucket:2 \n",
      " global step 2500 learning rate 0.4851 step-time 0.15 loss 43.6144 perplexity 8.74e+18\n",
      " step: 2600  step_loss: 38.0747 step_time:  0.1278 bucket:2 \n",
      " global step 2600 learning rate 0.4851 step-time 0.16 loss 45.6786 perplexity 6.89e+19\n",
      " step: 2700  step_loss: 23.6815 step_time:  0.0778 bucket:1 \n",
      " global step 2700 learning rate 0.4851 step-time 0.17 loss 46.3056 perplexity 1.29e+20\n",
      " step: 2800  step_loss: 37.1196 step_time:  0.1281 bucket:2 \n",
      " global step 2800 learning rate 0.4803 step-time 0.14 loss 39.7108 perplexity 1.76e+17\n",
      " step: 2900  step_loss: 39.5413 step_time:  0.1369 bucket:2 \n",
      " global step 2900 learning rate 0.4803 step-time 0.17 loss 45.0987 perplexity 3.86e+19\n",
      " step: 3000  step_loss: 36.1630 step_time:  0.1361 bucket:2 \n",
      " global step 3000 learning rate 0.4803 step-time 0.16 loss 42.7541 perplexity 3.70e+18\n",
      " step: 3100  step_loss: 25.2993 step_time:  0.0781 bucket:1 \n",
      " global step 3100 learning rate 0.4803 step-time 0.16 loss 43.5225 perplexity 7.97e+18\n",
      " step: 3200  step_loss: 39.0061 step_time:  0.1327 bucket:2 \n",
      " global step 3200 learning rate 0.4803 step-time 0.15 loss 41.0045 perplexity 6.43e+17\n",
      " step: 3300  step_loss: 36.0017 step_time:  0.1273 bucket:2 \n",
      " global step 3300 learning rate 0.4803 step-time 0.16 loss 43.9208 perplexity 1.19e+19\n",
      " step: 3400  step_loss: 68.9006 step_time:  0.2710 bucket:3 \n",
      " global step 3400 learning rate 0.4755 step-time 0.16 loss 42.6519 perplexity 3.34e+18\n",
      " step: 3500  step_loss: 38.3565 step_time:  0.1284 bucket:2 \n",
      " global step 3500 learning rate 0.4755 step-time 0.16 loss 42.5541 perplexity 3.03e+18\n",
      " step: 3600  step_loss: 36.7675 step_time:  0.1292 bucket:2 \n",
      " global step 3600 learning rate 0.4755 step-time 0.15 loss 38.3996 perplexity 4.75e+16\n",
      " step: 3700  step_loss: 65.0431 step_time:  0.2712 bucket:3 \n",
      " global step 3700 learning rate 0.4755 step-time 0.15 loss 40.4590 perplexity 3.72e+17\n",
      " step: 3800  step_loss: 20.7865 step_time:  0.0774 bucket:1 \n",
      " global step 3800 learning rate 0.4755 step-time 0.15 loss 39.3987 perplexity 1.29e+17\n",
      " step: 3900  step_loss: 34.2857 step_time:  0.1296 bucket:2 \n",
      " global step 3900 learning rate 0.4755 step-time 0.15 loss 39.5791 perplexity 1.55e+17\n",
      " step: 4000  step_loss: 38.1364 step_time:  0.1315 bucket:2 \n",
      " global step 4000 learning rate 0.4755 step-time 0.14 loss 35.7284 perplexity 3.29e+15\n"
     ]
    }
   ],
   "source": [
    "SEED = 0\n",
    "tf.set_random_seed(SEED)\n",
    "np.random.seed(SEED)\n",
    "with tf.Graph().as_default():\n",
    "    train(gen_config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "import bleu\n",
    "def eval(gen_config):\n",
    "    vocab, rev_vocab, dev_set, train_set = data_utils.prepare_data(gen_config)\n",
    "    for b_set in dev_set:\n",
    "        print(\"b_set: \", len(b_set))\n",
    "\n",
    "    with tf.Session() as sess:\n",
    "        model = Seq2SeqModel(gen_config, name_scope=\"Basic_Seq2seq\", forward_only=True,\n",
    "                                            dtype=tf.float32)\n",
    "        gen_ckpt_dir = os.path.abspath(os.path.join(gen_config.save_dir, \"checkpoints\"))\n",
    "        ckpt = tf.train.get_checkpoint_state(gen_ckpt_dir)\n",
    "        if ckpt and tf.train.checkpoint_exists(ckpt.model_checkpoint_path):\n",
    "            #print(\"Reading Gen model parameters from %s\" % ckpt.model_checkpoint_path)\n",
    "            model.saver.restore(sess, ckpt.model_checkpoint_path)\n",
    "        else: \n",
    "            raise ValueError(\"Please run the training first\")\n",
    "        # Run evals on development set and print their perplexity.\n",
    "        for bucket_id in xrange(len(gen_config.buckets)):\n",
    "            encoder_inputs, decoder_inputs, target_weights, inputs_len, target_len = data_utils.get_batch(\n",
    "                    gen_config, dev_set, bucket_id)\n",
    "            _, eval_loss, sample_ids = model.step(sess, encoder_inputs, decoder_inputs, target_weights, inputs_len, target_len, bucket_id, True)\n",
    "            eval_ppx = math.exp(eval_loss) if eval_loss < 300 else float('inf')\n",
    "            print(\"eval: bucket %d loss %.4f perplexity %.2e\" % (bucket_id, eval_loss, eval_ppx))\n",
    "            queries = data_utils.clean(encoder_inputs, data_utils.PAD_ID)\n",
    "            answers = data_utils.clean(decoder_inputs[1:], data_utils.EOS_ID)\n",
    "            gens = data_utils.clean(sample_ids, data_utils.EOS_ID)\n",
    "            references = [[gen] for gen in gens]\n",
    "            for i in range(4):\n",
    "                bleu_score, _, _, _, _, _ = bleu.compute_bleu(references, answers, max_order = i+1)\n",
    "                print(\"BLEU %d sorces: %.4f\"%(i+1, 100 * bleu_score))\n",
    "            for i in range(3):\n",
    "                print(\"Q:\", \" \".join([tf.compat.as_str(rev_vocab[j]) for j in queries[i]]))\n",
    "                print(\"A:\", \" \".join([tf.compat.as_str(rev_vocab[j]) for j in answers[i]]))\n",
    "                print(\"G:\", \" \".join([tf.compat.as_str(rev_vocab[j]) for j in gens[i]]))\n",
    "                bleu_score, _, _, _, _, _ = bleu.compute_bleu([[gens[i]]], [answers[i]], max_order = 1)\n",
    "                print(\"BLEU sorces: %.4f\"%(100 * bleu_score))\n",
    "                print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reading development and training gen_data\n",
      "b_set:  133\n",
      "b_set:  1427\n",
      "b_set:  3007\n",
      "b_set:  1826\n",
      "INFO:tensorflow:Restoring parameters from /home/chenminghao/git_work/Chatbot_test/log/gen_models/checkpoints/gen.model-4000\n",
      "eval: bucket 0 loss 59.9980 perplexity 1.14e+26\n",
      "BLEU 1 sorces: 16.7702\n",
      "BLEU 2 sorces: 4.3100\n",
      "BLEU 3 sorces: 1.8703\n",
      "BLEU 4 sorces: 1.1135\n",
      "Q: okay , tom .\n",
      "A: where ' s cindy ?\n",
      "G: what ' s the matter ?\n",
      "BLEU sorces: 49.1238\n",
      "\n",
      "Q: thanks a lot .\n",
      "A: not at all .\n",
      "G: you ' re welcome .\n",
      "BLEU sorces: 19.4700\n",
      "\n",
      "Q: great . bye .\n",
      "A: bye .\n",
      "G: you ' re welcome .\n",
      "BLEU sorces: 11.1565\n",
      "\n",
      "eval: bucket 1 loss 82.7694 perplexity 8.84e+35\n",
      "BLEU 1 sorces: 21.4082\n",
      "BLEU 2 sorces: 7.9135\n",
      "BLEU 3 sorces: 4.5553\n",
      "BLEU 4 sorces: 2.7409\n",
      "Q: no . just occasionally .\n",
      "A: what ' s your favorite dance ?\n",
      "G: you ' re welcome .\n",
      "BLEU sorces: 14.2857\n",
      "\n",
      "Q: i ' ll send a squad car .\n",
      "A: please hurry .\n",
      "G: ok .\n",
      "BLEU sorces: 33.3333\n",
      "\n",
      "Q: oh , so many kinds of winter hats .\n",
      "A: what is your favorite color , miss ?\n",
      "G: what are you doing ?\n",
      "BLEU sorces: 25.0000\n",
      "\n",
      "eval: bucket 2 loss 121.7435 perplexity 7.46e+52\n",
      "BLEU 1 sorces: 15.1125\n",
      "BLEU 2 sorces: 5.4455\n",
      "BLEU 3 sorces: 2.6334\n",
      "BLEU 4 sorces: 1.4704\n",
      "Q: ah . . . i see . hey , look the sign here , no climbing .\n",
      "A: we put many signs with english _UNK for foreign visitors .\n",
      "G: thank you .\n",
      "BLEU sorces: 9.0909\n",
      "\n",
      "Q: for instance , the mobile phone seriously distracts the attention of people from work and study .\n",
      "A: that ' s true . some people pay too much attention to their mobile phones .\n",
      "G: what are you going to do with the company ?\n",
      "BLEU sorces: 6.2500\n",
      "\n",
      "Q: not bad ! do you do any other sports ?\n",
      "A: not really . i â€™ d really like to try golf , though .\n",
      "G: i think i ' d better make a good idea .\n",
      "BLEU sorces: 21.4286\n",
      "\n",
      "eval: bucket 3 loss 229.1685 perplexity 3.36e+99\n",
      "BLEU 1 sorces: 11.9089\n",
      "BLEU 2 sorces: 4.4960\n",
      "BLEU 3 sorces: 2.2559\n",
      "BLEU 4 sorces: 1.0818\n",
      "Q: i need to find out some more information for l / c . i would like an outline of responsibilities , both ours , yours and the beneficiary , please .\n",
      "A: ok , to start , the conditions are _UNK by the buyer and may include insurance forms , way bills , bills of lading , customs forms , various certificates .\n",
      "G: that ' s a good idea .\n",
      "BLEU sorces: 3.2258\n",
      "\n",
      "Q: the documentary also pointed out that they are used by shepherds to round up sheep and by rescue workers to find people trapped under rubble or snow .\n",
      "A: horses are useful to people too . we use them for sports and recreation .\n",
      "G: what are you going to do with the food ?\n",
      "BLEU sorces: 13.3333\n",
      "\n",
      "Q: wow ! really ? are you sure ? now i ' ll have a lucky year !\n",
      "A: remember , real luck must be created ( by ) yourself , and then it cannot be given or taken away . . .\n",
      "G: you ' re right .\n",
      "BLEU sorces: 4.1667\n",
      "\n"
     ]
    }
   ],
   "source": [
    "with tf.Graph().as_default():\n",
    "    eval(gen_config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
